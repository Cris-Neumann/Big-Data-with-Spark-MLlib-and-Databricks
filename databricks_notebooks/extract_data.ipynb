{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de77c82-6d2e-4a03-9434-9986a5724d17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbfs:/FileStore/\ndbfs:/databricks-datasets/\ndbfs:/databricks-results/\ndbfs:/tmp/\n"
     ]
    }
   ],
   "source": [
    "# Lista el contenido del directorio raíz en DBFS (cluster distribuido, donde persisten los datos)\n",
    "root_files = dbutils.fs.ls('/')\n",
    "for file in root_files:\n",
    "    print(file.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "662927a7-e6e1-416f-8cea-cbe6e4e08d42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbfs:/FileStore/shared_uploads/cristian.neumann@usach.cl/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "# Lista el contenido del directorio /FileStore, donde se cargó el json con las credenciales de la API de Kaggle\n",
    "filestore_files = dbutils.fs.ls(\"/FileStore/shared_uploads/cristian.neumann@usach.cl\")\n",
    "for file in filestore_files:\n",
    "    print(file.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba389717-9823-4409-a1f6-d46373242641",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: '{\"username\":\"cristianneumann\",\"key\":\"51956eb6263a97a83b2c65c47447a264\"}'"
     ]
    }
   ],
   "source": [
    "# Leer el contenido de un archivo\n",
    "dbutils.fs.head(\"dbfs:/FileStore/shared_uploads/cristian.neumann@usach.cl/kaggle.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "523f6853-a8bf-4083-9a44-3ae200e6b2db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Instalar librerias de Kaggle\n",
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ebfc4a8-ae09-40ee-aa47-cead8173ead9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'username': 'YOUR_USER_NAME', 'key': 'YOUR_KEY'}\n"
     ]
    }
   ],
   "source": [
    "# Confirgurar credenciales de Kaggle\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Cargar el archivo kaggle.json que has subido a DBFS\n",
    "dbutils.fs.cp(\"dbfs:/FileStore/shared_uploads/cristian.neumann@usach.cl/kaggle.json\", \"file:/root/.kaggle/kaggle.json\")\n",
    "\n",
    "# Establecer los permisos adecuados\n",
    "os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n",
    "\n",
    "# Verificar que las credenciales estén configuradas correctamente\n",
    "with open(\"/root/.kaggle/kaggle.json\", \"r\") as f:\n",
    "    kaggle_credentials = json.load(f)\n",
    "    print(kaggle_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd20325-8151-4b1d-b586-8bcfeb86ffb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ruchi798/parquet-files-amexdefault-prediction 22GB\nodins0n/amex-parquet 9GB\nrobikscube/ubiquant-parquet 13GB\nraddar/amex-data-integer-dtypes-parquet-format 4GB\nrohanrao/riiid-train-data-multiple-formats 4GB\nrobikscube/ai4code-parquet-tabular 785MB\nravi20076/optiver-memoryreduceddatasets 2GB\njjinho/wikipedia-20230701 12GB\nalvinleenh/tps-rocket-league-data-float16-parquet-format 2GB\ncolumbia2131/otto-chunk-data-inparquet-format 2GB\nryati131457/riiid-parquet-files 564MB\nsamuelcortinhas/tps-nov-22-dataset-in-parquet-format 1GB\nmustafakeser4/tpsoct22-parquet 3GB\ncdeotte/otto-validation 1GB\ntitericz/leap-dataset-giba 16GB\npedrocouto39/jane-street-market-train-data-best-formats 10GB\n"
     ]
    }
   ],
   "source": [
    "# Listar los datasets de kaggle disponibles en su API\n",
    "import kaggle\n",
    "\n",
    "# Funcion que transforma el contenido de dataset.size (string) en número\n",
    "def size_to_bytes(size_str):\n",
    "    size_str = size_str.strip().upper()\n",
    "    size_units = {\"KB\": 1024, \"MB\": 1024**2, \"GB\": 1024**3, \"TB\": 1024**4}\n",
    "    if size_str[-2:] in size_units:\n",
    "        return float(size_str[:-2]) * size_units[size_str[-2:]]\n",
    "    elif size_str[-1] == \"B\":\n",
    "        return float(size_str[:-1])\n",
    "    else:\n",
    "        return float(size_str)\n",
    "\n",
    "# Buscar datasets grandes. Probamos con la etiqueta \"parquet\", pudiendo existir muchas otras.\n",
    "datasets = kaggle.api.dataset_list(search=\"parquet\")\n",
    "\n",
    "# Filtrar datasets por tamaño (mostrando aquellos mayores a 0,5 GB)\n",
    "large_datasets = [dataset for dataset in datasets if size_to_bytes(dataset.size) > (1024**3)/2]\n",
    "\n",
    "# Mostrar los datasets grandes\n",
    "for dataset in large_datasets:\n",
    "    print(dataset, dataset.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de3f82a-53aa-442f-8a73-6af486ea33ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raddar/amex-data-integer-dtypes-parquet-format] 4GB\n"
     ]
    }
   ],
   "source": [
    "# Buscar el dataset \"amex-data-integer-dtypes-parquet-format\" del usuario \"raddar\"\n",
    "dataset = kaggle.api.dataset_list(search=\"raddar/amex-data-integer-dtypes-parquet-format\")\n",
    "print(dataset, dataset[0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecccb58c-3c11-4e92-884d-d7f5727cbf17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/raddar/amex-data-integer-dtypes-parquet-format\n"
     ]
    }
   ],
   "source": [
    "# Importar el dataset \"amex-data-integer-dtypes-parquet-format\" en ruta del nodo local /tmp/wiki_data/. Esto, pues la API de Kaggle descargará el archivo al nodo local.\n",
    "import kaggle\n",
    "kaggle.api.dataset_download_files('raddar/amex-data-integer-dtypes-parquet-format', path='/tmp/wiki_data/', unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb06e4a4-cd24-4eab-9d23-08b3353473eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista el contenido del directorio Local: file:/tmp/wiki_data/\n",
    "root_local_files = dbutils.fs.ls('file:/tmp/wiki_data/')\n",
    "for file in root_local_files:\n",
    "    print(file.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f3290db-4e8a-4132-a1ae-9170bd28c859",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: True"
     ]
    }
   ],
   "source": [
    "# Mover el dataset descargado desde el sistema de archivos local al distribuido (DBFS), para que persista\n",
    "dbutils.fs.cp(\"file:/tmp/wiki_data/\", \"dbfs:/tmp/wiki_data/\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fdad2f8-c9af-47d4-9dbc-ec47b6fe13c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbfs:/FileStore/\ndbfs:/databricks-datasets/\ndbfs:/databricks-results/\ndbfs:/tmp/\n"
     ]
    }
   ],
   "source": [
    "# Volvemos a listar el contenido del directorio raíz en DBFS para verificar que aparezca el path del dataset copiado\n",
    "root_files = dbutils.fs.ls('/')\n",
    "for file in root_files:\n",
    "    print(file.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cec7c8a-8450-4b22-8fd7-9f63a5a5c03f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ACLARACIÓN \n",
    "- Local Filesystem: Cuando usas \"file:\", te estás refiriendo al sistema de archivos local del nodo en el que se está ejecutando tu código. Esto es similar a cómo manejarías archivos en un entorno de escritorio o servidor normal. Los archivos almacenados en el sistema de archivos local son temporales y pueden no estar disponibles después de que finaliza la sesión o el clúster se reinicia.\n",
    "\n",
    "- Databricks File System (dbfs:): DBFS es un sistema de archivos distribuido que está disponible en todos los nodos de tu clúster de Databricks. Esto significa que los archivos en DBFS pueden ser accedidos desde cualquier nodo, lo cual es crucial para la ejecución de trabajos distribuidos. Los archivos almacenados en DBFS son persistentes y están disponibles incluso después de que finaliza la sesión o se reinicia el clúster. Esto hace que DBFS sea ideal para almacenar datos que necesitas reutilizar o compartir entre sesiones. DBFS está optimizado para trabajar con Apache Spark, lo que facilita la lectura y escritura de grandes conjuntos de datos distribuidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51585e95-b958-41f2-bbed-4693d31c3941",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbfs:/tmp/wiki_data/test.parquet\ndbfs:/tmp/wiki_data/train.parquet\ndbfs:/tmp/wiki_data/train_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# Lista el contenido del directorio dbfs:/tmp/wiki_data/\n",
    "files = dbutils.fs.ls(\"dbfs:/tmp/wiki_data/\")\n",
    "for file in files:\n",
    "    print(file.path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "extract_data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
