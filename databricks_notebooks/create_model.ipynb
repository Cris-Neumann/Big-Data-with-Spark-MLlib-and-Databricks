{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de77c82-6d2e-4a03-9434-9986a5724d17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbfs:/FileStore/\ndbfs:/databricks-datasets/\ndbfs:/databricks-results/\ndbfs:/tmp/\n"
     ]
    }
   ],
   "source": [
    "# Lista el contenido del directorio raíz en DBFS (cluster distribuido, donde persisten los datos)\n",
    "root_files = dbutils.fs.ls('/')\n",
    "for file in root_files:\n",
    "    print(file.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "662927a7-e6e1-416f-8cea-cbe6e4e08d42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbfs:/FileStore/shared_uploads/cristian.neumann@usach.cl/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "# Lista el contenido del directorio /FileStore\n",
    "filestore_files = dbutils.fs.ls(\"/FileStore/shared_uploads/cristian.neumann@usach.cl\")\n",
    "for file in filestore_files:\n",
    "    print(file.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba389717-9823-4409-a1f6-d46373242641",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: '{\"username\":\"cristianneumann\",\"key\":\"51956eb6263a97a83b2c65c47447a264\"}'"
     ]
    }
   ],
   "source": [
    "# Leer el contenido de un archivo\n",
    "dbutils.fs.head(\"dbfs:/FileStore/shared_uploads/cristian.neumann@usach.cl/kaggle.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "523f6853-a8bf-4083-9a44-3ae200e6b2db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting kaggle\n  Downloading kaggle-1.6.14.tar.gz (82 kB)\nRequirement already satisfied: six>=1.10 in /databricks/python3/lib/python3.9/site-packages (from kaggle) (1.16.0)\nCollecting certifi>=2023.7.22\n  Downloading certifi-2024.7.4-py3-none-any.whl (162 kB)\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.9/site-packages (from kaggle) (2.8.2)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (from kaggle) (2.27.1)\nCollecting tqdm\n  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\nCollecting python-slugify\n  Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\nRequirement already satisfied: urllib3 in /databricks/python3/lib/python3.9/site-packages (from kaggle) (1.26.9)\nRequirement already satisfied: bleach in /databricks/python3/lib/python3.9/site-packages (from kaggle) (4.1.0)\nRequirement already satisfied: webencodings in /databricks/python3/lib/python3.9/site-packages (from bleach->kaggle) (0.5.1)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from bleach->kaggle) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->bleach->kaggle) (3.0.4)\nCollecting text-unidecode>=1.3\n  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests->kaggle) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests->kaggle) (2.0.4)\nBuilding wheels for collected packages: kaggle\n  Building wheel for kaggle (setup.py): started\n  Building wheel for kaggle (setup.py): finished with status 'done'\n  Created wheel for kaggle: filename=kaggle-1.6.14-py3-none-any.whl size=105136 sha256=4320da42e0bd471242ed7599a48ae1cd36ebf76d5d9a9faef60d2776c9b10464\n  Stored in directory: /root/.cache/pip/wheels/23/f1/ec/3f9079ada5cc1218f9accf05ccca9f0a57533191e49d76e3f6\nSuccessfully built kaggle\nInstalling collected packages: text-unidecode, certifi, tqdm, python-slugify, kaggle\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2021.10.8\n    Not uninstalling certifi at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-94424984-67b8-4e73-a0be-6365d8f47251\n    Can't uninstall 'certifi'. No files were found to uninstall.\nSuccessfully installed certifi-2024.7.4 kaggle-1.6.14 python-slugify-8.0.4 text-unidecode-1.3 tqdm-4.66.4\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "# Instalar librerias de Kaggle\n",
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ebfc4a8-ae09-40ee-aa47-cead8173ead9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'username': 'cristianneumann', 'key': '51956eb6263a97a83b2c65c47447a264'}\n"
     ]
    }
   ],
   "source": [
    "# Confirgurar credenciales de Kaggle\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Cargar el archivo kaggle.json que has subido a DBFS\n",
    "dbutils.fs.cp(\"dbfs:/FileStore/shared_uploads/cristian.neumann@usach.cl/kaggle.json\", \"file:/root/.kaggle/kaggle.json\")\n",
    "\n",
    "# Establecer los permisos adecuados\n",
    "os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n",
    "\n",
    "# Verificar que las credenciales estén configuradas correctamente\n",
    "with open(\"/root/.kaggle/kaggle.json\", \"r\") as f:\n",
    "    kaggle_credentials = json.load(f)\n",
    "    print(kaggle_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcd20325-8151-4b1d-b586-8bcfeb86ffb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ruchi798/parquet-files-amexdefault-prediction 22GB\nodins0n/amex-parquet 9GB\nrobikscube/ubiquant-parquet 13GB\nraddar/amex-data-integer-dtypes-parquet-format 4GB\nrohanrao/riiid-train-data-multiple-formats 4GB\nrobikscube/ai4code-parquet-tabular 785MB\nravi20076/optiver-memoryreduceddatasets 2GB\njjinho/wikipedia-20230701 12GB\nalvinleenh/tps-rocket-league-data-float16-parquet-format 2GB\ncolumbia2131/otto-chunk-data-inparquet-format 2GB\nryati131457/riiid-parquet-files 564MB\nsamuelcortinhas/tps-nov-22-dataset-in-parquet-format 1GB\nmustafakeser4/tpsoct22-parquet 3GB\ncdeotte/otto-validation 1GB\ntitericz/leap-dataset-giba 16GB\npedrocouto39/jane-street-market-train-data-best-formats 10GB\n"
     ]
    }
   ],
   "source": [
    "# Listar los datasets de kaggle disponibles en su API\n",
    "import kaggle\n",
    "\n",
    "# Funcion que transforma el contenido de dataset.size (string) en número\n",
    "def size_to_bytes(size_str):\n",
    "    size_str = size_str.strip().upper()\n",
    "    size_units = {\"KB\": 1024, \"MB\": 1024**2, \"GB\": 1024**3, \"TB\": 1024**4}\n",
    "    if size_str[-2:] in size_units:\n",
    "        return float(size_str[:-2]) * size_units[size_str[-2:]]\n",
    "    elif size_str[-1] == \"B\":\n",
    "        return float(size_str[:-1])\n",
    "    else:\n",
    "        return float(size_str)\n",
    "\n",
    "# Buscar datasets grandes. Probamos con la etiqueta \"parquet\", pudiendo existir muchas otras.\n",
    "datasets = kaggle.api.dataset_list(search=\"parquet\")\n",
    "\n",
    "# Filtrar datasets por tamaño (mostrando aquellos mayores a 0,5 GB)\n",
    "large_datasets = [dataset for dataset in datasets if size_to_bytes(dataset.size) > (1024**3)/2]\n",
    "\n",
    "# Mostrar los datasets grandes\n",
    "for dataset in large_datasets:\n",
    "    print(dataset, dataset.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de3f82a-53aa-442f-8a73-6af486ea33ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[raddar/amex-data-integer-dtypes-parquet-format] 4GB\n"
     ]
    }
   ],
   "source": [
    "# Buscar el dataset \"amex-data-integer-dtypes-parquet-format\" del usuario \"raddar\"\n",
    "dataset = kaggle.api.dataset_list(search=\"raddar/amex-data-integer-dtypes-parquet-format\")\n",
    "print(dataset, dataset[0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecccb58c-3c11-4e92-884d-d7f5727cbf17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/raddar/amex-data-integer-dtypes-parquet-format\n"
     ]
    }
   ],
   "source": [
    "# Importar el dataset \"amex-data-integer-dtypes-parquet-format\" en ruta del nodo local /tmp/wiki_data/. Esto, pues la API de Kaggle descargará el archivo al nodo local siempre.\n",
    "import kaggle\n",
    "kaggle.api.dataset_download_files('raddar/amex-data-integer-dtypes-parquet-format', path='/tmp/wiki_data/', unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb06e4a4-cd24-4eab-9d23-08b3353473eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista el contenido del directorio Local: file:/tmp/wiki_data/\n",
    "root_local_files = dbutils.fs.ls('file:/tmp/wiki_data/')\n",
    "for file in root_local_files:\n",
    "    print(file.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f3290db-4e8a-4132-a1ae-9170bd28c859",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: True"
     ]
    }
   ],
   "source": [
    "# Mover el dataset descargado desde el sistema de archivos local al distribuido (DBFS), para que persista\n",
    "dbutils.fs.cp(\"file:/tmp/wiki_data/\", \"dbfs:/tmp/wiki_data/\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fdad2f8-c9af-47d4-9dbc-ec47b6fe13c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbfs:/FileStore/\ndbfs:/databricks-datasets/\ndbfs:/databricks-results/\ndbfs:/tmp/\n"
     ]
    }
   ],
   "source": [
    "# Volvemos a listar el contenido del directorio raíz en DBFS para verificar que aparezca el path del dataset copiado\n",
    "root_files = dbutils.fs.ls('/')\n",
    "for file in root_files:\n",
    "    print(file.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cec7c8a-8450-4b22-8fd7-9f63a5a5c03f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ACLARACIÓN \n",
    "- Local Filesystem: Cuando usas \"file:\", te estás refiriendo al sistema de archivos local del nodo en el que se está ejecutando tu código. Esto es similar a cómo manejarías archivos en un entorno de escritorio o servidor normal. Los archivos almacenados en el sistema de archivos local son temporales y pueden no estar disponibles después de que finaliza la sesión o el clúster se reinicia.\n",
    "\n",
    "- Databricks File System (dbfs:): DBFS es un sistema de archivos distribuido que está disponible en todos los nodos de tu clúster de Databricks. Esto significa que los archivos en DBFS pueden ser accedidos desde cualquier nodo, lo cual es crucial para la ejecución de trabajos distribuidos. Los archivos almacenados en DBFS son persistentes y están disponibles incluso después de que finaliza la sesión o se reinicia el clúster. Esto hace que DBFS sea ideal para almacenar datos que necesitas reutilizar o compartir entre sesiones. DBFS está optimizado para trabajar con Apache Spark, lo que facilita la lectura y escritura de grandes conjuntos de datos distribuidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51585e95-b958-41f2-bbed-4693d31c3941",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbfs:/tmp/wiki_data/test.parquet\ndbfs:/tmp/wiki_data/train.parquet\n"
     ]
    }
   ],
   "source": [
    "# Lista el contenido del directorio dbfs:/tmp/wiki_data/\n",
    "files = dbutils.fs.ls(\"dbfs:/tmp/wiki_data/\")\n",
    "for file in files:\n",
    "    print(file.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb656925-2199-4741-a432-b89093f9b54c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cargar los datos de entrenamiento en un DataFrame de Spark\n",
    "path = \"dbfs:/tmp/wiki_data/train.parquet\"\n",
    "df_train = spark.read.format('parquet').options(header=True,inferSchema=True).load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af93b2cc-9237-46ff-b2c1-ed7b4aff2ca5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5531451, 190)\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos la cantidad de filas y columnas del DF de entrenamiento\n",
    "print((df_train.count(), len(df_train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4c8d780-e4af-440d-b341-63594d74529f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 179\n"
     ]
    }
   ],
   "source": [
    "# Conociendo el nombre de las variables categóricas, las separo de las variables numéricas.\n",
    "categoricas = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "numericas = [col for col in df_train.columns if col not in categoricas]\n",
    "print(len(categoricas), len(numericas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b474c15e-b21c-4c7e-a7ce-0c5eb3a369ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[30]: True"
     ]
    }
   ],
   "source": [
    "# Obtengo las etiquetas desde AWS S3.\n",
    "ACCESS_KEY = \"AKIA6NTIDLSP7UKVPAJJ\"\n",
    "SECRET_KEY = \"ExBmrC7VGh6MoBCIKkGdUQ5uvOev2uQbQmVPF7ZL\"\n",
    "ENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\n",
    "AWS_BUCKET_NAME = \"streaming-bucket-1\"\n",
    "S3_FILE_PATH = f\"s3a://{ACCESS_KEY}:{ENCODED_SECRET_KEY}@{AWS_BUCKET_NAME}/train_labels.csv\"\n",
    "DBFS_FILE_PATH = \"dbfs:/tmp/wiki_data/train_labels.csv\"\n",
    "\n",
    "# Copiar el archivo desde S3 a DBFS\n",
    "dbutils.fs.cp(S3_FILE_PATH, DBFS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb2d2bc-b218-416d-a931-cb060032e570",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbfs:/tmp/wiki_data/test.parquet\ndbfs:/tmp/wiki_data/train.parquet\ndbfs:/tmp/wiki_data/train_labels.csv\n"
     ]
    }
   ],
   "source": [
    "# Lista el contenido del directorio dbfs:/tmp/wiki_data/\n",
    "files = dbutils.fs.ls(\"dbfs:/tmp/wiki_data/\")\n",
    "for file in files:\n",
    "    print(file.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0409828-17e3-4127-a7d9-b04547243a3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n|         customer_ID|target|\n+--------------------+------+\n|0000099d6bd597052...|     0|\n|00000fd6641609c6e...|     0|\n|00001b22f846c82c5...|     0|\n+--------------------+------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo CSV con las etiquetas desde DBFS\n",
    "csv_file_path = \"dbfs:/tmp/wiki_data/train_labels.csv\"\n",
    "df_labels = spark.read.csv(csv_file_path, header=True, inferSchema=True, sep=\";\")\n",
    "\n",
    "# Mostrar los datos del DataFrame con las etiquetas\n",
    "df_labels.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07e56d79-5e2a-4639-baa5-79f5f1bbb824",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count of target variable\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.countplot(x=train_y.target)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "list_directory_test",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
